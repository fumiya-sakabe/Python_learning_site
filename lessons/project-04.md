# ãƒŸãƒ‹ã‚¢ãƒ—ãƒª: Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å…¥é–€

## ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚´ãƒ¼ãƒ«

- `requests`ã¨`BeautifulSoup`ã‚’ä½¿ã£ã¦HTMLã‚’å–å¾—ãƒ»è§£æã§ãã‚‹
- å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦CSV/JSONã«ä¿å­˜ã§ãã‚‹
- ç°¡å˜ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ä¾‹å¤–å‡¦ç†ã‚’å®Ÿè£…ã§ãã‚‹

## æ³¨æ„

- åˆ©ç”¨è¦ç´„/robots.txtã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„
- ã‚¢ã‚¯ã‚»ã‚¹ã®ç¤¼å„€ï¼ˆçŸ­ã„ãƒ‡ã‚£ãƒ¬ã‚¤ã€éå‰°ãƒªã‚¯ã‚¨ã‚¹ãƒˆç¦æ­¢ï¼‰

## æº–å‚™

```bash
pip install requests beautifulsoup4
```

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€å°ï¼‰

```python
import time
import csv
import requests
from bs4 import BeautifulSoup

URL = 'https://example.com'


def fetch(url: str) -> str:
    headers = {'User-Agent': 'Mozilla/5.0'}
    r = requests.get(url, headers=headers, timeout=10)
    r.raise_for_status()
    return r.text


def parse(html: str):
    soup = BeautifulSoup(html, 'html.parser')
    # ä¾‹: ã‚¿ã‚¤ãƒˆãƒ«ä¸€è¦§ã‚’æŠ½å‡º
    return [h.get_text(strip=True) for h in soup.select('h2')]


def main():
    html = fetch(URL)
    items = parse(html)
    with open('items.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['title'])
        for t in items:
            writer.writerow([t])

if __name__ == '__main__':
    main()
```

## æ¼”ç¿’èª²é¡Œ

1. ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’è¾¿ã£ã¦ã€è©³ç´°æƒ…å ±ï¼ˆä¾‹: è‘—è€…ãƒ»æ—¥ä»˜ï¼‰ã‚‚å–å¾—ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ã‚’è¦‹ã‚‹</summary>

è§£ç­”ï¼š

ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’åé›†ã—ã€å„è©³ç´°ãƒšãƒ¼ã‚¸ã‚’å–å¾—ã—ã¦è©³ç´°æƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

å®Ÿè£…ä¾‹ï¼š

```python
from urllib.parse import urljoin

def fetch_list_page(url: str):
    """ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    html = fetch(url)
    soup = BeautifulSoup(html, 'html.parser')
    
    # è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’åé›†
    detail_links = []
    for link in soup.select('a[href]'):
        href = link.get('href')
        if href:
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            absolute_url = urljoin(url, href)
            detail_links.append(absolute_url)
    
    return detail_links

def parse_detail(html: str):
    """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # ä¾‹: ã‚¿ã‚¤ãƒˆãƒ«ã€è‘—è€…ã€æ—¥ä»˜ã‚’æŠ½å‡º
    title = soup.select_one('h1').get_text(strip=True) if soup.select_one('h1') else 'N/A'
    author = soup.select_one('.author').get_text(strip=True) if soup.select_one('.author') else 'N/A'
    date = soup.select_one('.date').get_text(strip=True) if soup.select_one('.date') else 'N/A'
    
    return {
        'title': title,
        'author': author,
        'date': date
    }

def scrape_with_details(list_url: str):
    """ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ãƒšãƒ¼ã‚¸ã‚’è¾¿ã£ã¦æƒ…å ±ã‚’å–å¾—"""
    detail_links = fetch_list_page(list_url)
    
    results = []
    for detail_url in detail_links:
        html = fetch(detail_url)
        detail_info = parse_detail(html)
        detail_info['url'] = detail_url
        results.append(detail_info)
    
    return results

# ä½¿ç”¨ä¾‹
results = scrape_with_details('https://example.com/list')
for r in results:
    print(f"ã‚¿ã‚¤ãƒˆãƒ«: {r['title']}, è‘—è€…: {r['author']}, æ—¥ä»˜: {r['date']}")
```

ãƒã‚¤ãƒ³ãƒˆï¼š
- `soup.select('a[href]')`ã§ãƒªãƒ³ã‚¯ã‚’åé›†
- `urljoin(base_url, relative_url)`ã§ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
- å„è©³ç´°ãƒšãƒ¼ã‚¸ã‚’`fetch`ã—ã¦`parse_detail`ã§æƒ…å ±ã‚’æŠ½å‡º
- ã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆ`.author`, `.date`ãªã©ï¼‰ã¯å¯¾è±¡ã‚µã‚¤ãƒˆã«åˆã‚ã›ã¦èª¿æ•´

</details>

2. é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ã‚ˆã†ã«ã€1ã€œ2ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒªãƒ¼ãƒ—ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ã‚’è¦‹ã‚‹</summary>

è§£ç­”ï¼š

å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“ã«ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¹ãƒªãƒ¼ãƒ—ã‚’å…¥ã‚Œã¦ã€ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã«ã—ã¾ã™ã€‚

å®Ÿè£…ä¾‹ï¼š

```python
import random
import time

def fetch_with_delay(url: str, delay_min: float = 1.0, delay_max: float = 2.0):
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“ã«ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒªãƒ¼ãƒ—ã‚’å…¥ã‚Œã‚‹"""
    headers = {'User-Agent': 'Mozilla/5.0'}
    r = requests.get(url, headers=headers, timeout=10)
    r.raise_for_status()
    
    # 1ã€œ2ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒªãƒ¼ãƒ—
    sleep_time = random.uniform(delay_min, delay_max)
    time.sleep(sleep_time)
    
    return r.text

def scrape_with_delay(list_url: str):
    """é…å»¶ã‚’å…¥ã‚Œã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    detail_links = fetch_list_page(list_url)
    
    results = []
    for i, detail_url in enumerate(detail_links, 1):
        print(f"å‡¦ç†ä¸­ ({i}/{len(detail_links)}): {detail_url}")
        
        html = fetch_with_delay(detail_url)  # é…å»¶ã‚ã‚Š
        detail_info = parse_detail(html)
        results.append(detail_info)
    
    return results

# ä½¿ç”¨ä¾‹
results = scrape_with_delay('https://example.com/list')
```

ãƒã‚¤ãƒ³ãƒˆï¼š
- `random.uniform(1, 2)`ã§1ã€œ2ç§’ã®ãƒ©ãƒ³ãƒ€ãƒ ãªæ™‚é–“ã‚’ç”Ÿæˆ
- `time.sleep()`ã§å‡¦ç†ã‚’ä¸€æ™‚åœæ­¢
- å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å¾Œã«ã‚¹ãƒªãƒ¼ãƒ—ã‚’å…¥ã‚Œã‚‹
- é€²è¡ŒçŠ¶æ³ã‚’è¡¨ç¤ºã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¾…ã£ã¦ã„ã‚‹ã“ã¨ã‚’ä¼ãˆã‚‹

</details>

3. ä¾‹å¤–å‡¦ç†ã¨ãƒªãƒˆãƒ©ã‚¤ï¼ˆæœ€å¤§3å›ï¼‰ã‚’å°å…¥ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ã‚’è¦‹ã‚‹</summary>

è§£ç­”ï¼š

ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã©ã®ä¾‹å¤–ã‚’ã‚­ãƒ£ãƒƒãƒã—ã€æœ€å¤§3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™ã€‚

å®Ÿè£…ä¾‹ï¼š

```python
def fetch_with_retry(url: str, max_attempts: int = 3):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    for attempt in range(max_attempts):
        try:
            r = requests.get(url, headers=headers, timeout=10)
            r.raise_for_status()
            return r.text
        except requests.RequestException as e:
            if attempt < max_attempts - 1:
                print(f"âš ï¸ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ï¼ˆ{attempt + 1}å›ç›®ï¼‰: {e}")
                print(f"   1ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(1)
            else:
                print(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ: {url}")
                raise  # æœ€å¾Œã®è©¦è¡Œã§ã‚‚å¤±æ•—ã—ãŸã‚‰ä¾‹å¤–ã‚’å†ç™ºç”Ÿ
    
    return None

def scrape_with_retry(list_url: str):
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    detail_links = fetch_list_page(list_url)
    
    results = []
    for detail_url in detail_links:
        try:
            html = fetch_with_retry(detail_url)
            if html:
                detail_info = parse_detail(html)
                results.append(detail_info)
        except Exception as e:
            print(f"âš ï¸ {detail_url} ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {e}")
            continue  # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚ç¶šè¡Œ
    
    return results

# ä½¿ç”¨ä¾‹
results = scrape_with_retry('https://example.com/list')
```

ãƒã‚¤ãƒ³ãƒˆï¼š
- `for attempt in range(max_attempts)`ã§æœ€å¤§3å›ã¾ã§è©¦è¡Œ
- `try/except requests.RequestException`ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
- ãƒªãƒˆãƒ©ã‚¤å‰ã«1ç§’ã‚¹ãƒªãƒ¼ãƒ—ã—ã¦ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è»½æ¸›
- æœ€å¾Œã®è©¦è¡Œã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯ä¾‹å¤–ã‚’å†ç™ºç”Ÿ
- 1ã¤ã®URLã§ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚ã€ä»–ã®URLã®å‡¦ç†ã¯ç¶šè¡Œ

</details>

4. å–å¾—çµæœã‚’CSVã ã‘ã§ãªãJSONã«ã‚‚ä¿å­˜ã—ã¦ãã ã•ã„ã€‚

<details>
<summary>ğŸ’¡ è§£ç­”ã‚’è¦‹ã‚‹</summary>

è§£ç­”ï¼š

å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’CSVã¨JSONã®ä¸¡æ–¹ã®å½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚

å®Ÿè£…ä¾‹ï¼š

```python
import json

def save_to_csv(results, filename: str = 'items.csv'):
    """CSVå½¢å¼ã§ä¿å­˜"""
    if not results:
        return
    
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        fieldnames = list(results[0].keys())  # æœ€åˆã®è¦ç´ ã®ã‚­ãƒ¼ã‚’å–å¾—
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)
    
    print(f"âœ… CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")

def save_to_json(results, filename: str = 'items.json'):
    """JSONå½¢å¼ã§ä¿å­˜"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… JSONã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")

def main():
    html = fetch(URL)
    items = parse(html)
    
    # CSVã¨JSONã®ä¸¡æ–¹ã§ä¿å­˜
    save_to_csv(items, 'items.csv')
    save_to_json(items, 'items.json')

# ä½¿ç”¨ä¾‹
results = scrape_with_retry('https://example.com/list')
save_to_csv(results, 'results.csv')
save_to_json(results, 'results.json')
```

ãƒã‚¤ãƒ³ãƒˆï¼š
- `json.dump()`ã§JSONå½¢å¼ã§ä¿å­˜
- `ensure_ascii=False`ã§æ—¥æœ¬èªã‚’æ­£ã—ãä¿å­˜
- `indent=2`ã§è¦‹ã‚„ã™ã„å½¢å¼ã«æ•´å½¢
- CSVã¨JSONã®ä¸¡æ–¹ã§ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€ç”¨é€”ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘å¯èƒ½

</details>







